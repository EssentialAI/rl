<!-- #region -->
<span style="color:blue;">*You can navigate through this page quickly by using the navigation list on the right-hand side of this page.*</span>

#### What is Reinforcement Learning?

As mentioned in the Preface, <span style="color:blue;">"Reinforcement Learning is a field that deals with building models that mimic human behaviour to learn."</span> To completely understand how Reinforcement Learning (RL) models work, let's try and understand how humans learn.

Humans learn in a "cause and effect" fashion. For example, we humans do not know how to code right from birth. Learning to code is an interactive process. We make errors in the code, debug them, progressively gain experience about the consequences of actions and what actions to perform to achieve maximum success. _Learning from the interaction is a foundational idea underlying nearly all theories of learning and intelligence._

```{admonition} What is RL?
:class: tip

Reinforcement Learning is a field that aims to build models that try to achieve 'goal-directed learning from interaction' than other approaches to machine learning. <span style="color:blue;">Reinforcement learning (RL) tells you how to make the best decisions, sequentially, within a context, to maximize a real-life measure of success.</span>

**Learning by 'reinforcement' combines two tasks. The first is exploring new situations. The second is using that experience to make better decisions.**

_We shall discuss the concepts of exploration, exploitation, and optimal decisions in the # chapter._
```
```{admonition} Note
:class: idea

The characteristics of 'Trial-and-Error based search' and 'Delayed Reward' are the two most important distinguishing features of Reinforcement Learning.
```
_Differences between Machine Learning and Reinforcement Learning are discussed in detail below._

#### Why Reinforcement Learning?

One key feature of reinforcement learning is that it explicitly considers the *whole* problem of a goal-directed agent interacting with an uncertain environment. This is in contrast to many approaches that consider subproblems without addressing how they might fit into a larger picture.

Reinforcement learning is part of a decades-long trend within artificial intelligence and machine learning toward greater integration with statistics, optimization, and other mathematical subjects.

For example, the ability of some reinforcement learning methods to learn with parameterized approximators addresses the classical “curse of dimensionality” in operations research and control theory. More distinctively, reinforcement learning has also interacted strongly with psychology and neuroscience, with substantial benefits going both ways. Of all the forms of machine learning, reinforcement learning is the closest to the kind of learning that humans and other animals do, and many of the core algorithms of reinforcement learning were originally inspired by biological learning systems.

A good way to understand reinforcement learning is to consider some of the examples and possible applications that have guided its development.
* A master chess player makes a move. The choice is informed both by planning—anticipating possible replies and counter replies—and by immediate, intuitive judgments of the desirability of particular positions and moves.

* An adaptive controller adjusts parameters of a petroleum refinery’s operation in real time. The controller optimizes the yield/cost/quality trade-of on the basis of specified marginal costs without sticking strictly to the set points originally suggested by engineers.

* A gazelle calf struggles to its feet minutes after being born. Half an hour later it is running at 20 miles per hour.

* A mobile robot decides whether it should enter a new room in search of more trash to collect or start trying to find its way back to its battery recharging station. It makes its decision based on the current charge level of its battery and how quickly and easily it has been able to find the recharger in the past.


#### Machine Learning
```{admonition} Note
:class: idea

[Source](https://rl-book.com/) I consider ML a child of data science, which is an overarching scientific field that investigates the data generated by phenomena. I dislike the term artificial intelligence (AI) for a similar reason; it is hard enough to
define what intelligence is, let alone specify how it is achieved.
```
<span style="color:blue;">Supervised Machine Learning.</span>
In this type of Machine Learning, models learn to generalize from numerous training images provided with labels. In Supervised Machine Learning, the data is split into training and test datasets. Each example is a description of a situation together with a specification, the label, of the correct action the system should take to that situation, which is often to identify a category to which the situation belongs. 

The objective of this kind of learning is for the system to extrapolate or generalize its responses so that it acts correctly in situations not present in the training set.

<span style="color:blue;">Unsupervised Machine Learning.</span>
In this type of Machine Learning, models try to figure out patterns in the data without labels. Unsupervised learning is aimed at finding structures hidden in collections of unlabelled data.

#### Is RL same as ML?

Although one might be tempted to think of reinforcement learning as a kind of unsupervised learning (because it does not rely on examples of correct behavior), reinforcement learning tries to maximize a reward signal instead of trying to find a hidden structure.

Reinforcement learning takes a different track compared to Machine Learning, starting with a complete, interactive, goal-seeking agent. All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments.

When reinforcement learning involves planning, it has to address the interplay between planning and real-time action selection, as well as the question of how environment models are acquired and improved.

#### RL Fundamentals
<span style="color:blue;">Agent:</span>
An agent is a software program that learns to make intelligent decisions. For instance, a chess player can be considered an agent since the player learns to make the best moves (decisions) to win the game. Similarly, Mario in a Super Mario Bros video game can be considered an agent since Mario explores the game and learns to make the best moves in the game.

<span style="color:blue;">Environment:</span>
The environment is the world of the agent. The agent interacts within the environments. For example, a chessboard is called the environment for the chess player agent.

<span style="color:blue;">State and Action:</span>
A state is a position or a moment in the environment that the agent can be in. There can be many `positions in the chess board environment` that we discussed earlier. All these positions on the chess board are cosidered to be the state. The movement of the chess player agent (forward, backwward, right, and left) are known as actions. (A state is denoted by $s$, and an action is denoted by $a$).

<span style="color:blue;">Reward:</span>
As we discussed earlier, the agent interacts with the environments by performing actions. Every action has a reward associated to it. This reward is a numerical value, _(+1 or -1 for example)_ that denotes if the agent performed an optimal action or not.

The goal of the RL agent is to `maximize this reward` by performing an optimal set of actions.

<span style="color:blue;">Policy:</span>
A policy defines the learning agent’s way of behaving at a given time.

<span style="color:blue;">Value Functions:</span>
Whereas the reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state. Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward. Rewards are basically given directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime.

<span style="color:blue;">Model-based and Model-free methods:</span>
Methods for solving reinforcement learning problems that use models and planning are called model-based methods, as opposed to simpler model-free methods that are explicitly trial-and-error learners—viewed as almost the opposite of planning.
<!-- #endregion -->
